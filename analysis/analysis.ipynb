{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f41a7063",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "from tld import get_fld\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from IPython.display import HTML\n",
    "import urllib.parse as parse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bdaa31d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATA_DIR = '../crawl_data'\n",
    "SRC_DIR = '../crawler_src'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0a20050",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "os.chdir(DATA_DIR)\n",
    "\n",
    "# Get desktop json files\n",
    "data_json_desktop = glob.glob('*_desktop.json')\n",
    "\n",
    "# Get mobile json files\n",
    "data_json_mobile = glob.glob('*_mobile.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9997ec4f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def init_data_object():\n",
    "    return  {\n",
    "        # Per url data\n",
    "        'tranco_ranks': [],\n",
    "        'page_load_times': [],\n",
    "        'num_requests': [],\n",
    "        'distinct_third_parties': [],\n",
    "        'num_distinct_tracker_domains': [],\n",
    "        'num_distinct_tracker_entities': [],\n",
    "        \n",
    "        # Global data\n",
    "        'failures': {\n",
    "            'timeout_failures': 0,\n",
    "            'TLS_failures': 0,\n",
    "            'consent_failures': 0\n",
    "        },\n",
    "        'third_party_counts': {},\n",
    "        'third_party_tracker_counts': {},\n",
    "        'third_party_tracker_entities': {},\n",
    "        'uber_cookie': {\n",
    "            'request_hostname': '',\n",
    "            'website': '',\n",
    "            'num_cookies': 0,\n",
    "            'first_party': False\n",
    "        },\n",
    "        'longest_lifespan_cookies': [],\n",
    "        'canvas_fingerprints': [],\n",
    "        'tracker_redirect_combos': []\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1dfe9699",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def parse_stupid_blocklist_to_something_readable(file_path):\n",
    "    url_list = {}\n",
    "    \n",
    "    with open(file_path, encoding='utf-8') as blocklist_file:\n",
    "        blocklist = json.load(blocklist_file)\n",
    "        \n",
    "        for cat, entities in blocklist['categories'].items():\n",
    "            for entity_list in entities:\n",
    "                for entity, url_objects in entity_list.items():\n",
    "                    for url, aliases in url_objects.items():\n",
    "                        all_urls = [url]\n",
    "                        all_urls += aliases\n",
    "                        \n",
    "                        if entity not in url_list:\n",
    "                            url_list[entity] = []\n",
    "                            \n",
    "                        for u in all_urls:\n",
    "                            \n",
    "                            try:\n",
    "                                url_list[entity].append(get_fld(u, fix_protocol=True))\n",
    "                            except:\n",
    "                                pass\n",
    "                        \n",
    "                        url_list[entity] = list(set(url_list[entity]))\n",
    "    \n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4953277a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_stats_object(json_files):\n",
    "    data_object = init_data_object()\n",
    "    \n",
    "    for json_file in tqdm(json_files):\n",
    "        with open(json_file, 'r', encoding='utf-8') as data_file:\n",
    "            try:\n",
    "                data = json.load(data_file)\n",
    "            except:\n",
    "                print(f'Error opening json file: {json_file}, skipping')\n",
    "                data = {\"website_domain\": json_file.split('_')[0],\n",
    "                        \"rank\": -1,\n",
    "                        \"crawl_mode\": \"desktop\",\n",
    "                        \"post_pageload_url\": None,\n",
    "                        \"pageload_start_ts\": -1,\n",
    "                        \"pageload_end_ts\": -1,\n",
    "                        \"consent_status\": None,\n",
    "                        \"requests\": [],\n",
    "                        \"load_time\": -1,\n",
    "                        \"cookies\": -1,\n",
    "                        \"canvas_image_data\": None,\n",
    "                        \"failure_status\": {\n",
    "                            \"timeout\": True,\n",
    "                            \"TLS\": None,\n",
    "                            \"consent\": False\n",
    "                        }\n",
    "                    }\n",
    "                \n",
    "            # Append tranco rank\n",
    "            data_object['tranco_ranks'].append(int(data['rank']))\n",
    "            \n",
    "            # Updata failure counts\n",
    "            if data['failure_status']['timeout']:\n",
    "                data_object['failures']['timeout_failures'] += 1\n",
    "            if data['failure_status']['TLS'] != 'null':\n",
    "                data_object['failures']['TLS_failures'] += 1\n",
    "            if data['failure_status']['consent']:\n",
    "                data_object['failures']['consent_failures'] += 1\n",
    "                \n",
    "            # Only proceed if there is no timeout\n",
    "            if data['failure_status']['timeout']:\n",
    "                data_object['num_requests'].append(None)\n",
    "                data_object['page_load_times'].append(None)\n",
    "                data_object['distinct_third_parties'].append(None)\n",
    "                data_object['num_distinct_tracker_domains'].append(None)\n",
    "                data_object['num_distinct_tracker_entities'].append(None)\n",
    "                continue\n",
    "            \n",
    "            # Append page load time\n",
    "            data_object['page_load_times'].append(data['load_time'])\n",
    "            \n",
    "            # Append number of requests\n",
    "            data_object['num_requests'].append(len(data['requests']))\n",
    "            \n",
    "            # Append distinct third parties\n",
    "            def get_fld_websocket(u: str):\n",
    "                if u.startswith('wss://'):\n",
    "                    return get_fld(u[6:], fix_protocol=True)\n",
    "                if re.match('\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}', parse.urlparse(u).netloc) is not None:\n",
    "                    return u\n",
    "                else:\n",
    "                    return get_fld(u, fix_protocol=True)\n",
    "            distinct_third_parties = set([ get_fld_websocket(d['request_url'])\n",
    "                                           for d in data['requests']\n",
    "                                         ])\n",
    "            distinct_third_parties.remove(get_fld(data['website_domain'], fix_protocol=True))\n",
    "            data_object['distinct_third_parties'].append(len(distinct_third_parties))\n",
    "            \n",
    "            # Append number of distinct tracker domains\n",
    "            tracker_dict = parse_stupid_blocklist_to_something_readable(SRC_DIR + '/disconnectmeblocklist.json')\n",
    "            distinct_tracker_domains = []\n",
    "            for third_party_domain in distinct_third_parties:\n",
    "                for _, domains in tracker_dict.items():\n",
    "                    if third_party_domain in domains:\n",
    "                        distinct_tracker_domains.append(third_party_domain)\n",
    "            data_object['num_distinct_tracker_domains'].append(len(distinct_tracker_domains))\n",
    "            \n",
    "            # Append number of distinct tracker entities/companies\n",
    "            distinct_tracker_entities = []\n",
    "            with open(SRC_DIR + '/domain_map.json', encoding='utf-8') as domain_map_json_file:\n",
    "                domain_map_json = json.load(domain_map_json_file)\n",
    "                \n",
    "                for tracker_domain in distinct_tracker_domains:\n",
    "                    if tracker_domain in domain_map_json.keys():\n",
    "                        distinct_tracker_entities.append(domain_map_json[tracker_domain]['entityName'])\n",
    "                    else:\n",
    "                        for tracker_entity, tracker_entity_domains in tracker_dict.items():\n",
    "                            if tracker_domain in tracker_entity_domains:\n",
    "                                distinct_tracker_entities.append(tracker_entity)\n",
    "                                break\n",
    "                \n",
    "            distinct_tracker_entities = set(distinct_tracker_entities)\n",
    "            data_object['num_distinct_tracker_entities'].append(len(distinct_tracker_entities))\n",
    "            \n",
    "            # Update third party reference counts\n",
    "            for party in distinct_third_parties:\n",
    "                if party in data_object['third_party_counts']:\n",
    "                    data_object['third_party_counts'][party] += 1\n",
    "                else:\n",
    "                    data_object['third_party_counts'][party] = 1\n",
    "            \n",
    "            # Update third party tracker counts\n",
    "            for tracker in distinct_tracker_domains:\n",
    "                if tracker in data_object['third_party_tracker_counts']:\n",
    "                    data_object['third_party_tracker_counts'][tracker] += 1\n",
    "                else:\n",
    "                    data_object['third_party_tracker_counts'][tracker] =1\n",
    "            \n",
    "            # Update third party tracker entities\n",
    "            for entity in distinct_tracker_entities:\n",
    "                if entity in data_object['third_party_tracker_entities']:\n",
    "                    data_object['third_party_tracker_entities'][entity] += 1\n",
    "                else:\n",
    "                    data_object['third_party_tracker_entities'][entity] =1\n",
    "                    \n",
    "            # Update the uber cookie\n",
    "            max_cookie_count = None\n",
    "            request_url = None\n",
    "            for request in data['requests']:\n",
    "                if 'cookie' not in request['request_headers']:\n",
    "                    continue\n",
    "                    \n",
    "                cookie_count = len(request['request_headers']['cookie'].split(';'))\n",
    "                if max_cookie_count is None or cookie_count > max_cookie_count:\n",
    "                    max_cookie_count = cookie_count\n",
    "                    request_url = request['request_url']\n",
    "            \n",
    "            if max_cookie_count is not None and \\\n",
    "               max_cookie_count > data_object['uber_cookie']['num_cookies']:\n",
    "                data_object['uber_cookie']['num_cookies'] = max_cookie_count\n",
    "                data_object['uber_cookie']['request_hostname'] = get_fld(request_url, fix_protocol=True)\n",
    "                data_object['uber_cookie']['website'] = data['website_domain']\n",
    "                data_object['uber_cookie']['first_party'] = data_object['uber_cookie']['request_hostname'] == \\\n",
    "                                                            data_object['uber_cookie']['website']\n",
    "                \n",
    "            # Get longest lasting cookies\n",
    "            cookie_ids = []\n",
    "            for request in data['requests']:\n",
    "                if 'cookie' not in request['request_headers']:\n",
    "                    continue\n",
    "                    \n",
    "                cookies = request['request_headers']['cookie'].split(';')\n",
    "                for cookie in cookies:\n",
    "                    cookie_ids.append(cookie.split('=')[0])\n",
    "            \n",
    "            all_cookies = []\n",
    "            for cookie in data['cookies']:\n",
    "                if cookie['name'] in cookie_ids:\n",
    "                    cookie_data = cookie.copy()\n",
    "                    cookie_data['size'] = len(cookie_data['value'])\n",
    "                    \n",
    "                    if 'sameSite' not in cookie_data:\n",
    "                        cookie_data['sameSite'] = None\n",
    "\n",
    "                    if 'expiry' not in cookie_data: # Cookies without expiry exist\n",
    "                        cookie_data['expiry'] = 999999999999999\n",
    "                    all_cookies.append(cookie_data)\n",
    "                    all_cookies.append(cookie_data)\n",
    "\n",
    "            sort_alg = lambda c: c['expiry']\n",
    "            all_cookies.sort(key=sort_alg, reverse=True)\n",
    "            data_object['longest_lifespan_cookies'] += all_cookies[:3]\n",
    "            data_object['longest_lifespan_cookies'].sort(key=sort_alg, reverse=True)\n",
    "            data_object['longest_lifespan_cookies'] = data_object['longest_lifespan_cookies'][:3]\n",
    "            \n",
    "            # HTTP redirect pairs for tracker domains\n",
    "            tracker_redirect_combos = []\n",
    "            for request in data['requests']:\n",
    "                if request['response_status_code'] >= 300 and \\\n",
    "                   request['response_status_code'] <= 399 and \\\n",
    "                   'location' in request['response_headers']:\n",
    "                    origin_domain = get_fld(request['request_url'], fix_protocol=True)\n",
    "                    if request['response_headers']['location'].startswith('http'): # TODO: verify this filters exactly all non-relative URLs\n",
    "                        redirect_domain = get_fld(request['response_headers']['location'], fix_protocol=True)\n",
    "                    else: # location can be relative URL\n",
    "                        redirect_domain = origin_domain\n",
    "                    if origin_domain != redirect_domain and \\\n",
    "                       (origin_domain in distinct_tracker_domains or \\\n",
    "                        redirect_domain in distinct_tracker_domains):\n",
    "                        tracker_redirect_combos.append((origin_domain, redirect_domain))\n",
    "            data_object['tracker_redirect_combos'] += list(set(tracker_redirect_combos))\n",
    "            \n",
    "            # Fingerprints\n",
    "            for fingerprint in data['canvas_image_data']:\n",
    "                fingerprint['website'] = get_fld(data['website_domain'], fix_protocol=True)\n",
    "            data_object['canvas_fingerprints'] += data['canvas_image_data']\n",
    "            \n",
    "    return data_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f156626",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 477/477 [00:31<00:00, 15.31it/s]\n",
      "100%|█████████████████████████████████████████████████████████████| 480/480 [00:36<00:00, 12.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create the data objects\n",
    "crawls = {\n",
    "    'desktop': create_stats_object(data_json_desktop),\n",
    "    'mobile': create_stats_object(data_json_mobile)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7589984d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_url_df(crawl_data):\n",
    "    slice_keys = ['tranco_ranks', 'page_load_times', 'num_requests', 'distinct_third_parties',\n",
    "              'num_distinct_tracker_domains', 'num_distinct_tracker_entities']\n",
    "    df_input = {k: v for k, v in crawl_data.items() if k in slice_keys}\n",
    "    return pd.DataFrame(data=df_input)\n",
    "\n",
    "df_desktop = get_url_df(crawls['desktop'])\n",
    "df_mobile = get_url_df(crawls['mobile'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794a1dc8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#1\n",
    "desktop_failures = {'Page load timeout': crawls['desktop']['failures']['timeout_failures'],\n",
    "                    'TLS error': crawls['desktop']['failures']['TLS_failures'],\n",
    "                    'Consent click error': crawls['desktop']['failures']['consent_failures']}\n",
    "\n",
    "mobile_failures = {'Page load timeout': crawls['mobile']['failures']['timeout_failures'],\n",
    "                    'TLS error': crawls['mobile']['failures']['TLS_failures'],\n",
    "                    'Consent click error': crawls['mobile']['failures']['consent_failures']}\n",
    "\n",
    "failures_table = pd.DataFrame({'Crawl-desktop': desktop_failures, 'Crawl-mobile': mobile_failures})\n",
    "failures_table.columns.name = 'Error type'\n",
    "print(failures_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c0d5fc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#2\n",
    "def draw_boxplot(metric):\n",
    "    box_df = pd.DataFrame(df_desktop, columns=[metric])\n",
    "    box_df.rename(columns={metric:'Crawl-desktop'}, inplace=True)\n",
    "    box_df = box_df.join(df_mobile[metric])\n",
    "    box_df.rename(columns={metric:'Crawl-mobile'}, inplace=True)\n",
    "    return sns.boxplot(data=pd.DataFrame(data=box_df, columns=['Crawl-desktop','Crawl-mobile']))\n",
    "\n",
    "g_pageload = draw_boxplot('page_load_times')\n",
    "g_requests = draw_boxplot('num_requests')\n",
    "g_thirdparties = draw_boxplot('distinct_third_parties')\n",
    "g_trackdomains = draw_boxplot('num_distinct_tracker_domains')\n",
    "g_trackentities = draw_boxplot('num_distinct_tracker_entities')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed2532c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#4,5,6\n",
    "def print_ten_most_prev_websites_table(desktop_counts, mobile_counts):\n",
    "    table_desktop = pd.DataFrame({'Third-party desktop': desktop_counts.keys(), 'Nr of websites desktop': desktop_counts.values()}).sort_values(by=['Nr of websites desktop'],ascending=False).reset_index(drop=True)\n",
    "    table_mobile = pd.DataFrame({'Third-party mobile': mobile_counts.keys(), 'Nr of websites mobile': mobile_counts.values()}).sort_values(by=['Nr of websites mobile'],ascending=False).reset_index(drop=True)\n",
    "\n",
    "    table = pd.concat([table_desktop, table_mobile],axis=1)\n",
    "\n",
    "    # Some settings to make the table prettier\n",
    "    pd.set_option('display.precision', 0) # No decimals in count\n",
    "    pd.set_option('expand_frame_repr', False)\n",
    "    table.index += 1 # Start at index 1\n",
    "    columns=[('Crawl-desktop','Third-party'),('Crawl-desktop','Number of websites'),('Crawl-mobile','Third-party'),('Crawl-mobile','Number of websites') ]\n",
    "    table.columns=pd.MultiIndex.from_tuples(columns) # Add nested column names\n",
    "\n",
    "    #Only print the first 10 rows\n",
    "    print(table.head(10))\n",
    "\n",
    "print_ten_most_prev_websites_table(crawls['desktop']['third_party_counts'], crawls['mobile']['third_party_counts']) #4\n",
    "print()\n",
    "print_ten_most_prev_websites_table(crawls['desktop']['third_party_tracker_counts'], crawls['mobile']['third_party_tracker_counts'])#5\n",
    "print()\n",
    "print_ten_most_prev_websites_table(crawls['desktop']['third_party_tracker_entities'], crawls['mobile']['third_party_tracker_entities']) #6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b5ef0b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#7\n",
    "distinct_third_parties_desktop = pd.DataFrame( {'Tranco rank': crawls['desktop']['tranco_ranks'], 'Distinct third-party domains': crawls['desktop']['distinct_third_parties']})\n",
    "distinct_third_parties_mobile = pd.DataFrame( {'Tranco rank': crawls['mobile']['tranco_ranks'], 'Distinct third-party domains': crawls['mobile']['distinct_third_parties']})\n",
    "\n",
    "# TODO: remove NaN values\n",
    "\n",
    "sns.lmplot(x='Tranco rank', y='Distinct third-party domains', data=distinct_third_parties_desktop)\n",
    "sns.lmplot(x='Tranco rank', y='Distinct third-party domains', data=distinct_third_parties_mobile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b43d73",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#8\n",
    "distinct_trackers_desktop = pd.DataFrame( {'Tranco rank': crawls['desktop']['tranco_ranks'], 'Distinct tracker domains': crawls['desktop']['num_distinct_tracker_domains']})\n",
    "distinct_trackers_mobile = pd.DataFrame( {'Tranco rank': crawls['mobile']['tranco_ranks'], 'Distinct tracker domains': crawls['mobile']['num_distinct_tracker_domains']})\n",
    "\n",
    "# TODO: remove NaN values\n",
    "\n",
    "sns.lmplot(x='Tranco rank', y='Distinct tracker domains', data=distinct_trackers_desktop)\n",
    "sns.lmplot(x='Tranco rank', y='Distinct tracker domains', data=distinct_trackers_mobile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb86790a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#9\n",
    "data_9 = []\n",
    "for crawl, crawl_data in crawls.items():\n",
    "    data_9.append([crawl] + list(crawl_data['uber_cookie'].values()))\n",
    "    \n",
    "df_9 = pd.DataFrame(data_9)\n",
    "df_9.columns = ['crawl'] + list(crawls['desktop']['uber_cookie'].keys())\n",
    "df_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce23abb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#10\n",
    "data_10 = []\n",
    "for crawl, crawl_data in crawls.items():\n",
    "    for cookie in crawl_data['longest_lifespan_cookies']:\n",
    "        data_10.append([crawl] + list(cookie.values()))\n",
    "\n",
    "df_10 = pd.DataFrame(data_10)\n",
    "df_10.columns = ['crawl'] + list(crawls['desktop']['longest_lifespan_cookies'][0].keys())\n",
    "df_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c822e79",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#11\n",
    "data_11 = []\n",
    "for crawl, crawl_data in crawls.items():\n",
    "    for combo in crawl_data['tracker_redirect_combos']:\n",
    "        data_11.append([crawl, combo])\n",
    "        \n",
    "df_11 = pd.DataFrame(data_11)\n",
    "df_11.columns = ['crawl', 'combo']\n",
    "df_11.groupby(['crawl','combo'])['combo'] \\\n",
    "     .count() \\\n",
    "     .reset_index(name=\"count\") \\\n",
    "     .sort_values(['crawl', 'count'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3da273",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12\n",
    "canvas_fingerprints = []\n",
    "for crawl, crawl_data in crawls.items():\n",
    "    for fingerprint in crawl_data['canvas_fingerprints']:\n",
    "        canvas_fingerprints.append([crawl,\n",
    "                                    fingerprint['website'],\n",
    "                                    fingerprint['fingerprint_script_resource_url'],\n",
    "                                    '<img src=\"' + DATA_DIR + '/' + fingerprint['canvas_fingerprint_image'] + '\" />'\n",
    "                                   ])\n",
    "df_canvas_fingerprints = pd.DataFrame(canvas_fingerprints)\n",
    "df_canvas_fingerprints.columns = ['Crawl', 'Website', 'Fingerprint script URL', 'Canvas image']\n",
    "df_canvas_fingerprints = df_canvas_fingerprints.to_html(escape=False)\n",
    "HTML(df_canvas_fingerprints)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
