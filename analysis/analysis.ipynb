{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from tld import get_fld\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from IPython.display import HTML"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATA_DIR = '../crawl_data'\n",
    "SRC_DIR = '../crawler_src'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "os.chdir(DATA_DIR)\n",
    "\n",
    "# Get desktop json files\n",
    "data_json_desktop = glob.glob('*_desktop.json')\n",
    "\n",
    "# Get mobile json files\n",
    "data_json_mobile = glob.glob('*_mobile.json')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def init_data_object():\n",
    "    return  {\n",
    "        # Per url data\n",
    "        'tranco_ranks': [],\n",
    "        'page_load_times': [],\n",
    "        'num_requests': [],\n",
    "        'distinct_third_parties': [],\n",
    "        'num_distinct_tracker_domains': [],\n",
    "        'num_distinct_tracker_entities': [],\n",
    "        \n",
    "        # Global data\n",
    "        'failures': {\n",
    "            'timeout_failures': 0,\n",
    "            'TLS_failures': 0,\n",
    "            'consent_failures': 0\n",
    "        },\n",
    "        'third_party_counts': {},\n",
    "        'third_party_tracker_counts': {},\n",
    "        'third_party_tracker_entities': {},\n",
    "        'uber_cookie': {\n",
    "            'request_hostname': '',\n",
    "            'website': '',\n",
    "            'num_cookies': 0,\n",
    "            'first_party': False\n",
    "        },\n",
    "        'longest_lifespan_cookies': [],\n",
    "        'canvas_fingerprints': [],\n",
    "        'tracker_redirect_combos': []\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parse_stupid_blocklist_to_something_readable(file_path):\n",
    "    url_list = {}\n",
    "    \n",
    "    with open(file_path, encoding='utf-8') as blocklist_file:\n",
    "        blocklist = json.load(blocklist_file)\n",
    "        \n",
    "        for cat, entities in blocklist['categories'].items():\n",
    "            for entity_list in entities:\n",
    "                for entity, url_objects in entity_list.items():\n",
    "                    for url, aliases in url_objects.items():\n",
    "                        all_urls = [url]\n",
    "                        all_urls += aliases\n",
    "                        \n",
    "                        if entity not in url_list:\n",
    "                            url_list[entity] = []\n",
    "                            \n",
    "                        for u in all_urls:\n",
    "                            \n",
    "                            try:\n",
    "                                url_list[entity].append(get_fld(u, fix_protocol=True))\n",
    "                            except:\n",
    "                                pass\n",
    "                        \n",
    "                        url_list[entity] = list(set(url_list[entity]))\n",
    "    \n",
    "    return url_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_stats_object(json_files):\n",
    "    data_object = init_data_object()\n",
    "    \n",
    "    for json_file in tqdm(json_files):\n",
    "        with open(json_file, 'r', encoding='utf-8') as data_file:\n",
    "            try:\n",
    "                data = json.load(data_file)\n",
    "            except:\n",
    "                print(f'Error opening json file: {json_file}, skipping')\n",
    "                data = {\"website_domain\": json_file.split('_')[0],\n",
    "                        \"rank\": -1,\n",
    "                        \"crawl_mode\": \"desktop\",\n",
    "                        \"post_pageload_url\": None,\n",
    "                        \"pageload_start_ts\": -1,\n",
    "                        \"pageload_end_ts\": -1,\n",
    "                        \"consent_status\": None,\n",
    "                        \"requests\": [],\n",
    "                        \"load_time\": -1,\n",
    "                        \"cookies\": -1,\n",
    "                        \"canvas_image_data\": None,\n",
    "                        \"failure_status\": {\n",
    "                            \"timeout\": True,\n",
    "                            \"TLS\": None,\n",
    "                            \"consent\": False\n",
    "                        }\n",
    "                    }\n",
    "                # TODO: whatsapp.com_desktop.json has no contents\n",
    "            # Append tranco rank\n",
    "            data_object['tranco_ranks'].append(data['rank'])\n",
    "            \n",
    "            # Updata failure counts\n",
    "            if data['failure_status']['timeout']:\n",
    "                data_object['failures']['timeout_failures'] += 1\n",
    "            if data['failure_status']['TLS'] != 'null':\n",
    "                data_object['failures']['TLS_failures'] += 1\n",
    "            if data['failure_status']['consent']:\n",
    "                data_object['failures']['consent_failures'] += 1\n",
    "                \n",
    "            # Only proceed if there is no timeout\n",
    "            if data['failure_status']['timeout']:\n",
    "                data_object['num_requests'].append(None)\n",
    "                data_object['page_load_times'].append(None)\n",
    "                data_object['distinct_third_parties'].append(None)\n",
    "                data_object['num_distinct_tracker_domains'].append(None)\n",
    "                data_object['num_distinct_tracker_entities'].append(None)\n",
    "                continue\n",
    "            \n",
    "            # Append page load time\n",
    "            data_object['page_load_times'].append(data['load_time'])\n",
    "            \n",
    "            # Append number of requests\n",
    "            data_object['num_requests'].append(len(data['requests']))\n",
    "            \n",
    "            # Append distinct third parties\n",
    "            get_fld_websocket = lambda u: get_fld(u[6:], fix_protocol=True) if u.startswith('wss://') \\\n",
    "                else get_fld(u, fix_protocol=True)\n",
    "            distinct_third_parties = set([ get_fld_websocket(d['request_url'])\n",
    "                                           for d in data['requests']\n",
    "                                         ])\n",
    "            distinct_third_parties.remove(get_fld(data['website_domain'], fix_protocol=True))\n",
    "            data_object['distinct_third_parties'].append(len(distinct_third_parties))\n",
    "            \n",
    "            # Append number of distinct tracker domains\n",
    "            tracker_dict = parse_stupid_blocklist_to_something_readable(SRC_DIR + '/disconnectmeblocklist.json')\n",
    "            distinct_tracker_domains = []\n",
    "            for third_party_domain in distinct_third_parties:\n",
    "                for _, domains in tracker_dict.items():\n",
    "                    if third_party_domain in domains:\n",
    "                        distinct_tracker_domains.append(third_party_domain)\n",
    "            data_object['num_distinct_tracker_domains'].append(len(distinct_tracker_domains))\n",
    "            \n",
    "            # Append number of distinct tracker entities/companies\n",
    "            distinct_tracker_entities = []\n",
    "            with open(SRC_DIR + '/domain_map.json', encoding='utf-8') as domain_map_json_file:\n",
    "                domain_map_json = json.load(domain_map_json_file)\n",
    "                \n",
    "                for third_tracker_domain in distinct_tracker_domains:\n",
    "                    if third_tracker_domain in domain_map_json.keys():\n",
    "                        distinct_tracker_entities.append(domain_map_json[third_tracker_domain]['entityName'])\n",
    "                    else:\n",
    "                        print('DOMAIN NOT FOUND IN ENTITY LIST, IMPLEMENT THIS WHEN YOU SEE THIS! DOMAIN OF DOOM: {}'\n",
    "                              .format(third_party_domain))\n",
    "                        # TODO: implement this\n",
    "                \n",
    "            distinct_tracker_entities = set(distinct_tracker_entities)\n",
    "            data_object['num_distinct_tracker_entities'].append(len(distinct_tracker_entities))\n",
    "            \n",
    "            # Update third party reference counts\n",
    "            for party in distinct_third_parties:\n",
    "                if party in data_object['third_party_counts']:\n",
    "                    data_object['third_party_counts'][party] += 1\n",
    "                else:\n",
    "                    data_object['third_party_counts'][party] = 1\n",
    "            \n",
    "            # Update third party tracker counts\n",
    "            for tracker in distinct_tracker_domains:\n",
    "                if tracker in data_object['third_party_tracker_counts']:\n",
    "                    data_object['third_party_tracker_counts'][tracker] += 1\n",
    "                else:\n",
    "                    data_object['third_party_tracker_counts'][tracker] =1\n",
    "            \n",
    "            # Update third party tracker entities\n",
    "            for entity in distinct_tracker_entities:\n",
    "                if entity in data_object['third_party_tracker_entities']:\n",
    "                    data_object['third_party_tracker_entities'][entity] += 1\n",
    "                else:\n",
    "                    data_object['third_party_tracker_entities'][entity] =1\n",
    "                    \n",
    "            # Update the uber cookie\n",
    "            max_cookie_count = None\n",
    "            request_url = None\n",
    "            for request in data['requests']:\n",
    "                if 'cookie' not in request['request_headers']:\n",
    "                    continue\n",
    "                    \n",
    "                cookie_count = len(request['request_headers']['cookie'].split(';'))\n",
    "                if max_cookie_count is None or cookie_count > max_cookie_count:\n",
    "                    max_cookie_count = cookie_count\n",
    "                    request_url = request['request_url']\n",
    "            \n",
    "            if max_cookie_count is not None and \\\n",
    "               max_cookie_count > data_object['uber_cookie']['num_cookies']:\n",
    "                data_object['uber_cookie']['num_cookies'] = max_cookie_count\n",
    "                data_object['uber_cookie']['request_hostname'] = get_fld(request_url, fix_protocol=True)\n",
    "                data_object['uber_cookie']['website'] = data['website_domain']\n",
    "                data_object['uber_cookie']['first_party'] = data_object['uber_cookie']['request_hostname'] == \\\n",
    "                                                            data_object['uber_cookie']['website']\n",
    "                \n",
    "            # Get longest lasting cookies\n",
    "            cookie_ids = []\n",
    "            for request in data['requests']:\n",
    "                if 'cookie' not in request['request_headers']:\n",
    "                    continue\n",
    "                    \n",
    "                cookies = request['request_headers']['cookie'].split(';')\n",
    "                for cookie in cookies:\n",
    "                    cookie_ids.append(cookie.split('=')[0])\n",
    "            \n",
    "            all_cookies = []\n",
    "            for cookie in data['cookies']:\n",
    "                if cookie['name'] in cookie_ids:\n",
    "                    cookie_data = cookie.copy()\n",
    "                    cookie_data['size'] = len(cookie_data['value'])\n",
    "                    \n",
    "                    if 'sameSite' not in cookie_data:\n",
    "                        cookie_data['sameSite'] = None\n",
    "\n",
    "                    if 'expiry' not in cookie_data: # Cookies without expiry exist\n",
    "                        cookie_data['expiry'] = 999999999999999\n",
    "                    all_cookies.append(cookie_data)\n",
    "                    all_cookies.append(cookie_data)\n",
    "\n",
    "                sort_alg = lambda c: c['expiry']\n",
    "                all_cookies.sort(key=sort_alg, reverse=True)\n",
    "            data_object['longest_lifespan_cookies'] += all_cookies[:3]\n",
    "            data_object['longest_lifespan_cookies'].sort(key=sort_alg, reverse=True)\n",
    "            data_object['longest_lifespan_cookies'] = data_object['longest_lifespan_cookies'][:3]\n",
    "            \n",
    "            # HTTP redirect pairs for tracker domains\n",
    "            tracker_redirect_combos = []\n",
    "            for request in data['requests']:\n",
    "                if request['response_status_code'] >= 300 and \\\n",
    "                   request['response_status_code'] <= 399 and \\\n",
    "                   'location' in request['response_headers']:\n",
    "                    origin_domain = get_fld(request['request_url'], fix_protocol=True)\n",
    "                    if request['response_headers']['location'].startswith('http'): # TODO: verify this filters exactly all non-relative URLs\n",
    "                        redirect_domain = get_fld(request['response_headers']['location'], fix_protocol=True)\n",
    "                    else: # location can be relative URL\n",
    "                        redirect_domain = origin_domain\n",
    "                    if origin_domain != redirect_domain and \\\n",
    "                       (origin_domain in distinct_tracker_domains or \\\n",
    "                        redirect_domain in distinct_tracker_domains):\n",
    "                        tracker_redirect_combos.append((origin_domain, redirect_domain))\n",
    "            data_object['tracker_redirect_combos'] = list(set(tracker_redirect_combos))\n",
    "            \n",
    "            # Fingerprints\n",
    "            for fingerprint in data['canvas_image_data']:\n",
    "                fingerprint['website'] = get_fld(data['website_domain'], fix_protocol=True)\n",
    "            data_object['canvas_fingerprints'] += data['canvas_image_data']\n",
    "            \n",
    "    return data_object"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create the data objects\n",
    "crawls = {\n",
    "    'desktop': create_stats_object(data_json_desktop),\n",
    "    'mobile': create_stats_object(data_json_mobile)\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_url_df(crawl_data):\n",
    "    slice_keys = ['tranco_ranks', 'page_load_times', 'num_requests', 'distinct_third_parties',\n",
    "              'num_distinct_tracker_domains', 'num_distinct_tracker_entities']\n",
    "    df_input = {k: v for k, v in crawl_data.items() if k in slice_keys}\n",
    "    return pd.DataFrame(data=df_input)\n",
    "\n",
    "df_desktop = get_url_df(crawls['desktop'])\n",
    "df_mobile = get_url_df(crawls['mobile'])\n",
    "\n",
    "df_desktop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def draw_boxplot(metric):\n",
    "    box_df = pd.DataFrame(df_desktop, columns=[metric])\n",
    "    box_df.rename(columns={metric:'Crawl-desktop'}, inplace=True)\n",
    "    box_df = box_df.join(df_mobile[metric])\n",
    "    box_df.rename(columns={metric:'Crawl-mobile'}, inplace=True)\n",
    "    return sns.boxplot(data=pd.DataFrame(data=box_df, columns=['Crawl-desktop','Crawl-mobile']))\n",
    "\n",
    "g_pageload = draw_boxplot('page_load_times')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "g_requests = draw_boxplot('num_requests')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "g_thirdparties = draw_boxplot('distinct_third_parties')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "g_trackdomains = draw_boxplot('num_distinct_tracker_domains')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "g_trackentities = draw_boxplot('num_distinct_tracker_entities')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#1\n",
    "desktop_failures = {'Page load timeout': crawls['desktop']['failures']['timeout_failures'],\n",
    "                    'TLS error': crawls['desktop']['failures']['TLS_failures'],\n",
    "                    'Consent click error': crawls['desktop']['failures']['consent_failures']}\n",
    "\n",
    "mobile_failures = {'Page load timeout': crawls['mobile']['failures']['timeout_failures'],\n",
    "                    'TLS error': crawls['mobile']['failures']['TLS_failures'],\n",
    "                    'Consent click error': crawls['mobile']['failures']['consent_failures']}\n",
    "\n",
    "failures_table = pd.DataFrame({'Crawl-desktop': desktop_failures, 'Crawl-mobile': mobile_failures})\n",
    "failures_table.columns.name = 'Error type'\n",
    "print(failures_table)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Third-party desktop  Nr of websites desktop   Third-party mobile  \\\n",
      "0       doubleclick.net                      68               vk.com   \n",
      "1  google-analytics.com                      65      addthisedge.com   \n",
      "2  googletagmanager.com                      57          pstatic.net   \n",
      "3          facebook.com                      42         crazyegg.com   \n",
      "4            criteo.com                      11        altmetric.com   \n",
      "5             qhimg.com                       2  steelhousemedia.com   \n",
      "6             qhmsg.com                       1          shopify.com   \n",
      "7       creativecdn.com                       1      4seeresults.com   \n",
      "8              tmall.ru                       1       digitalgov.gov   \n",
      "9         aliexpress.ru                       1              b2c.com   \n",
      "\n",
      "   Nr of websites mobile  \n",
      "0                      1  \n",
      "1                      1  \n",
      "2                      1  \n",
      "3                      1  \n",
      "4                      1  \n",
      "5                      1  \n",
      "6                      1  \n",
      "7                      1  \n",
      "8                      1  \n",
      "9                      1   \n",
      "\n",
      "    Third-party desktop  Nr of websites desktop Third-party mobile  \\\n",
      "0       creativecdn.com                       1      semantiqo.com   \n",
      "1          facebook.com                      42          avocet.io   \n",
      "2  google-analytics.com                      65      smartlook.com   \n",
      "3            criteo.com                      11             vk.com   \n",
      "4       doubleclick.net                      68   staticflickr.com   \n",
      "5             google.nl                      47  smartadserver.com   \n",
      "6            google.com                      72       affinity.com   \n",
      "7          facebook.net                      35    siftscience.com   \n",
      "8         go-mpulse.net                       6     ebaystatic.com   \n",
      "9   amazon-adsystem.com                      15      recaptcha.net   \n",
      "\n",
      "   Nr of websites mobile  \n",
      "0                      1  \n",
      "1                      1  \n",
      "2                      1  \n",
      "3                      1  \n",
      "4                      1  \n",
      "5                      1  \n",
      "6                      1  \n",
      "7                      1  \n",
      "8                      1  \n",
      "9                      1   \n",
      "\n",
      "         Third-party desktop  Nr of websites desktop  \\\n",
      "0             Facebook, Inc.                      45   \n",
      "1                 Google LLC                     106   \n",
      "2        Akamai Technologies                       6   \n",
      "3                  Criteo SA                      11   \n",
      "4             RTB House S.A.                       1   \n",
      "5  Amazon Technologies, Inc.                      17   \n",
      "6                 Yandex LLC                       3   \n",
      "7            MediaMath, Inc.                       8   \n",
      "8           Optimizely, Inc.                       7   \n",
      "9            Permutive, Inc.                       4   \n",
      "\n",
      "              Third-party mobile  Nr of websites mobile  \n",
      "0  Rambler Internet Holding, LLC                      1  \n",
      "1                     RELX Group                      1  \n",
      "2         Simplifi Holdings Inc.                      1  \n",
      "3             Sift Science, Inc.                      1  \n",
      "4               Pulsepoint, Inc.                      1  \n",
      "5                OnAudience Ltd.                      1  \n",
      "6                LLC \"AmberData\"                      1  \n",
      "7                  SmugMug, Inc.                      1  \n",
      "8                   Centro, Inc.                      1  \n",
      "9                  BidTheatre AB                      1  \n"
     ]
    }
   ],
   "source": [
    "#4,5,6\n",
    "def ten_most_prev_websites_table(desktop_counts, mobile_counts):\n",
    "    table_desktop = pd.DataFrame({'Third-party desktop': desktop_counts.keys(), 'Nr of websites desktop': desktop_counts.values()})\n",
    "    table_mobile = pd.DataFrame({'Third-party mobile': mobile_counts.keys(), 'Nr of websites mobile': mobile_counts.values()}).sort_values('Nr of websites mobile').reset_index(drop=True)\n",
    "\n",
    "    table = pd.concat([table_desktop, table_mobile],axis=1)\n",
    "\n",
    "    # Some settings to make the table prettier\n",
    "    #pd.set_option('precision', 0) # No decimals in count\n",
    "    #table.fillna(\"\",inplace=True) # Replace 'None'  with ''\n",
    "    #table.index += 1 # Start at index 1\n",
    "    #columns=[('Crawl-desktop','Third-party'),('Crawl-desktop','Number of websites'),('Crawl-mobile','Third-party'),('Crawl-mobile','Number of websites') ]\n",
    "    #table.columns=pd.MultiIndex.from_tuples(columns) # Add nested column names\n",
    "\n",
    "    #Only print the first 10 rows\n",
    "    return table.head(10)\n",
    "\n",
    "print(ten_most_prev_websites_table(crawls['desktop']['third_party_counts'], crawls['mobile']['third_party_counts']).sort_values(by=['Nr of websites desktop'],ascending=False).reset_index(drop=True), \"\\n\") #4\n",
    "print(ten_most_prev_websites_table(crawls['desktop']['third_party_tracker_counts'], crawls['mobile']['third_party_tracker_counts']), \"\\n\") #5\n",
    "print(ten_most_prev_websites_table(crawls['desktop']['third_party_tracker_entities'], crawls['mobile']['third_party_tracker_entities'])) #6"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#12\n",
    "canvas_fingerprints = []\n",
    "for crawl, crawl_data in crawls.items():\n",
    "    for fingerprint in crawl_data['canvas_fingerprints']:\n",
    "        canvas_fingerprints.append([crawl,\n",
    "                                    fingerprint['website'],\n",
    "                                    fingerprint['fingerprint_script_resource_url'],\n",
    "                                    '<img src=\"' + DATA_DIR + '/' + fingerprint['canvas_fingerprint_image'] + '\" />'\n",
    "                                   ])\n",
    "df_canvas_fingerprints = pd.DataFrame(canvas_fingerprints)\n",
    "df_canvas_fingerprints.columns = ['Crawl', 'Website', 'Fingerprint script URL', 'Canvas image']\n",
    "df_canvas_fingerprints = df_canvas_fingerprints.to_html(escape=False)\n",
    "HTML(df_canvas_fingerprints)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}