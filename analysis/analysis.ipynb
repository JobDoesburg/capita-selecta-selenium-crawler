{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-92-b7d635602cc4>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mjson\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[1;32mfrom\u001B[0m \u001B[0mtld\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mget_fld\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mseaborn\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0msns\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      6\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mpandas\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "from tld import get_fld\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATA_DIR = '../crawl_data'\n",
    "SRC_DIR = '../crawler_src'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "os.chdir(DATA_DIR)\n",
    "\n",
    "# Get desktop json files\n",
    "data_json_desktop = glob.glob('*_desktop.json')\n",
    "\n",
    "# Get mobile json files\n",
    "data_json_mobile = glob.glob('*_mobile.json')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def init_data_object():\n",
    "    return  {\n",
    "        # Per url data\n",
    "        'tranco_ranks': [],\n",
    "        'page_load_times': [],\n",
    "        'num_requests': [],\n",
    "        'distinct_third_parties': [],\n",
    "        'num_distinct_tracker_domains': [],\n",
    "        'num_distinct_tracker_entities': [],\n",
    "        \n",
    "        # Global data\n",
    "        'failures': {\n",
    "            'timeout_failures': 0,\n",
    "            'TLS_failures': 0,\n",
    "            'consent_failures': 0\n",
    "        },\n",
    "        'third_party_counts': {},\n",
    "        'third_party_tracker_counts': {},\n",
    "        'third_party_tracker_entities': {},\n",
    "        'uber_cookie': {\n",
    "            'request_hostname': '',\n",
    "            'website': '',\n",
    "            'num_cookies': 0,\n",
    "            'first_party': False\n",
    "        },\n",
    "        'longest_lifespan_cookies': [],\n",
    "        'canvas_fingerprints': [],\n",
    "        'tracker_redirect_combos': []\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def parse_stupid_blocklist_to_something_readable(file_path):\n",
    "    url_list = {}\n",
    "    \n",
    "    with open(file_path, encoding='utf-8') as blocklist_file:\n",
    "        blocklist = json.load(blocklist_file)\n",
    "        \n",
    "        for cat, entities in blocklist['categories'].items():\n",
    "            for entity_list in entities:\n",
    "                for entity, url_objects in entity_list.items():\n",
    "                    for url, aliases in url_objects.items():\n",
    "                        all_urls = [url]\n",
    "                        all_urls += aliases\n",
    "                        \n",
    "                        if entity not in url_list:\n",
    "                            url_list[entity] = []\n",
    "                            \n",
    "                        for u in all_urls:\n",
    "                            \n",
    "                            try:\n",
    "                                url_list[entity].append(get_fld(u, fix_protocol=True))\n",
    "                            except:\n",
    "                                pass\n",
    "                        \n",
    "                        url_list[entity] = list(set(url_list[entity]))\n",
    "    \n",
    "    return url_list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_stats_object(json_files):\n",
    "    data_object = init_data_object()\n",
    "    \n",
    "    for json_file in json_files:\n",
    "        with open(json_file, 'r', encoding='utf-8') as data_file:\n",
    "            data = json.load(data_file)\n",
    "            \n",
    "            # Append tranco rank\n",
    "            data_object['tranco_ranks'].append(data['rank'])\n",
    "            \n",
    "            # Updata failure counts\n",
    "            if data['failure_status']['timeout']:\n",
    "                data_object['failures']['timeout_failures'] += 1\n",
    "            if data['failure_status']['TLS'] != 'null':\n",
    "                data_object['failures']['TLS_failures'] += 1\n",
    "            if data['failure_status']['consent']:\n",
    "                data_object['failures']['consent_failures'] += 1\n",
    "                \n",
    "            # Only proceed if there is no timeout\n",
    "            if data['failure_status']['timeout']:\n",
    "                data['num_requests'].append(None)\n",
    "                data['distinct_third_parties'].append(None)\n",
    "                data['num_distinct_tracker_domains'].append(None)\n",
    "                data['num_distinct_tracker_entities'].append(None)\n",
    "                continue\n",
    "            \n",
    "            # Append page load time\n",
    "            data_object['page_load_times'].append(data['load_time'])\n",
    "            \n",
    "            # Append number of requests\n",
    "            data_object['num_requests'].append(len(data['requests']))\n",
    "            \n",
    "            # Append distinct third parties\n",
    "            distinct_third_parties = set([ get_fld(d['request_url'], fix_protocol=True)\n",
    "                                           for d in data['requests']\n",
    "                                         ])\n",
    "            distinct_third_parties.remove(get_fld(data['website_domain'], fix_protocol=True))\n",
    "            data_object['distinct_third_parties'].append(len(distinct_third_parties))\n",
    "            \n",
    "            # Append number of distinct tracker domains\n",
    "            tracker_dict = parse_stupid_blocklist_to_something_readable(SRC_DIR + '/disconnectmeblocklist.json')\n",
    "            distinct_tracker_domains = []\n",
    "            for third_party_domain in distinct_third_parties:\n",
    "                for _, domains in tracker_dict.items():\n",
    "                    if third_party_domain in domains:\n",
    "                        distinct_tracker_domains.append(third_party_domain)\n",
    "            data_object['num_distinct_tracker_domains'].append(len(distinct_tracker_domains))\n",
    "            \n",
    "            # Append number of distinct tracker entities/companies\n",
    "            distinct_tracker_entities = []\n",
    "            with open(SRC_DIR + '/domain_map.json', encoding='utf-8') as domain_map_json_file:\n",
    "                domain_map_json = json.load(domain_map_json_file)\n",
    "                \n",
    "                for third_tracker_domain in distinct_tracker_domains:\n",
    "                    if third_tracker_domain in domain_map_json.keys():\n",
    "                        distinct_tracker_entities.append(domain_map_json[third_tracker_domain]['entityName'])\n",
    "                    else:\n",
    "                        print('DOMAIN NOT FOUND IN ENTITY LIST, IMPLEMENT THIS WHEN YOU SEE THIS! DOMAIN OF DOOM: {}'\n",
    "                              .format(third_party_domain))\n",
    "                \n",
    "            distinct_tracker_entities = set(distinct_tracker_entities)\n",
    "            data_object['num_distinct_tracker_entities'].append(len(distinct_tracker_entities))\n",
    "            \n",
    "            # Update third party reference counts\n",
    "            for party in distinct_third_parties:\n",
    "                if party in data_object['third_party_counts']:\n",
    "                    data_object['third_party_counts'][party] += 1\n",
    "                else:\n",
    "                    data_object['third_party_counts'][party] = 1\n",
    "            \n",
    "            # Update third party tracker counts\n",
    "            for tracker in distinct_tracker_domains:\n",
    "                if tracker in data_object['third_party_tracker_counts']:\n",
    "                    data_object['third_party_tracker_counts'][tracker] += 1\n",
    "                else:\n",
    "                    data_object['third_party_tracker_counts'][tracker] =1\n",
    "            \n",
    "            # Update third party tracker entities\n",
    "            for entity in distinct_tracker_entities:\n",
    "                if entity in data_object['third_party_tracker_entities']:\n",
    "                    data_object['third_party_tracker_entities'][entity] += 1\n",
    "                else:\n",
    "                    data_object['third_party_tracker_entities'][entity] =1\n",
    "                    \n",
    "            # Update the uber cookie\n",
    "            max_cookie_count = None\n",
    "            request_url = None\n",
    "            for request in data['requests']:\n",
    "                if 'cookie' not in request['request_headers']:\n",
    "                    continue\n",
    "                    \n",
    "                cookie_count = len(request['request_headers']['cookie'].split(';'))\n",
    "                if max_cookie_count is None or cookie_count > max_cookie_count:\n",
    "                    max_cookie_count = cookie_count\n",
    "                    request_url = request['request_url']\n",
    "            \n",
    "            if max_cookie_count is not None and \\\n",
    "               max_cookie_count > data_object['uber_cookie']['num_cookies']:\n",
    "                data_object['uber_cookie']['num_cookies'] = max_cookie_count\n",
    "                data_object['uber_cookie']['request_hostname'] = get_fld(request_url, fix_protocol=True)\n",
    "                data_object['uber_cookie']['website'] = data['website_domain']\n",
    "                data_object['uber_cookie']['first_party'] = data_object['uber_cookie']['request_hostname'] == \\\n",
    "                                                            data_object['uber_cookie']['website']\n",
    "                \n",
    "            # Get longest lasting cookies\n",
    "            cookie_ids = []\n",
    "            for request in data['requests']:\n",
    "                if 'cookie' not in request['request_headers']:\n",
    "                    continue\n",
    "                    \n",
    "                cookies = request['request_headers']['cookie'].split(';')\n",
    "                for cookie in cookies:\n",
    "                    cookie_ids.append(cookie.split('=')[0])\n",
    "            \n",
    "            all_cookies = []\n",
    "            for cookie in data['cookies']:\n",
    "                if cookie['name'] in cookie_ids:\n",
    "                    cookie_data = cookie.copy()\n",
    "                    cookie_data['size'] = len(cookie_data['value'])\n",
    "                    \n",
    "                    if 'sameSite' not in cookie_data:\n",
    "                        cookie_data['sameSite'] = None\n",
    "                        \n",
    "                    all_cookies.append(cookie_data)\n",
    "            \n",
    "            sort_alg = lambda c: c['expiry']\n",
    "            all_cookies.sort(key=sort_alg, reverse=True)\n",
    "            data_object['longest_lifespan_cookies'] += all_cookies[:3]\n",
    "            data_object['longest_lifespan_cookies'].sort(key=sort_alg, reverse=True)\n",
    "            data_object['longest_lifespan_cookies'] = data_object['longest_lifespan_cookies'][:3]\n",
    "            \n",
    "            # HTTP redirect pairs for tracker domains\n",
    "            tracker_redirect_combos = []\n",
    "            for request in data['requests']:\n",
    "                if request['response_status_code'] >= 300 and \\\n",
    "                   request['response_status_code'] <= 399 and \\\n",
    "                   'location' in request['response_headers']:\n",
    "                    origin_domain = get_fld(request['request_url'], fix_protocol=True)\n",
    "                    redirect_domain = get_fld(request['response_headers']['location'], fix_protocol=True)\n",
    "                    if origin_domain != redirect_domain and \\\n",
    "                       (origin_domain in distinct_tracker_domains or \\\n",
    "                        redirect_domain in distinct_tracker_domains):\n",
    "                        tracker_redirect_combos.append((origin_domain, redirect_domain))\n",
    "            data_object['tracker_redirect_combos'] = list(set(tracker_redirect_combos))\n",
    "            \n",
    "            # Fingerprints\n",
    "            for fingerprint in data['canvas_image_data']:\n",
    "                fingerprint['website'] = get_fld(data['website_domain'], fix_protocol=True)\n",
    "            data_object['canvas_fingerprints'] += data['canvas_image_data']\n",
    "            \n",
    "    return data_object"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create the data objects\n",
    "desktop_data = create_stats_object(data_json_desktop)\n",
    "mobile_data = create_stats_object(data_json_mobile)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_url_df(crawl_data):\n",
    "    slice_keys = ['tranco_ranks', 'page_load_times', 'num_requests', 'distinct_third_parties',\n",
    "              'num_distinct_tracker_domains', 'num_distinct_tracker_entities']\n",
    "    df_input = {k: v for k, v in crawl_data.items() if k in slice_keys}\n",
    "    return pd.DataFrame(data=df_input)\n",
    "\n",
    "df_desktop = get_url_df(desktop_data)\n",
    "df_mobile = get_url_df(mobile_data)\n",
    "\n",
    "df_desktop"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.boxplot(data=pd.DataFrame(data=df_desktop, columns=['tranco_ranks','page_load_times']))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#12\n",
    "canvas_data = []\n",
    "for crawl_data in []:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error type           Crawl-desktop  Crawl-mobile\n",
      "Page load timeout                0             0\n",
      "TLS error                        1             1\n",
      "Consent click error              1             0\n"
     ]
    }
   ],
   "source": [
    "# import pandas as pd\n",
    "import pandas as pd\n",
    "stats_object_desktop = create_stats_object(data_json_desktop)\n",
    "stats_object_mobile = create_stats_object(data_json_mobile)\n",
    "\n",
    "\n",
    "desktop_failures = {'Page load timeout': stats_object_desktop['failures']['timeout_failures'],\n",
    "                    'TLS error': stats_object_desktop['failures']['TLS_failures'],\n",
    "                    'Consent click error': stats_object_desktop['failures']['consent_failures']}\n",
    "\n",
    "mobile_failures = {'Page load timeout': stats_object_mobile['failures']['timeout_failures'],\n",
    "                    'TLS error': stats_object_mobile['failures']['TLS_failures'],\n",
    "                    'Consent click error': stats_object_mobile['failures']['consent_failures']}\n",
    "\n",
    "failures_table = pd.DataFrame({'Crawl-desktop': desktop_failures, 'Crawl-mobile': mobile_failures})\n",
    "failures_table.columns.name = 'Error type'\n",
    "print(failures_table)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "outputs": [
    {
     "data": {
      "text/plain": "                                            Crawl-desktop   Crawl-mobile\nThird-party domain  (youtube.com, google.nl, gstatic.com)  (gstatic.com)\nNumber of websites                              (1, 1, 1)            (1)",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Crawl-desktop</th>\n      <th>Crawl-mobile</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Third-party domain</th>\n      <td>(youtube.com, google.nl, gstatic.com)</td>\n      <td>(gstatic.com)</td>\n    </tr>\n    <tr>\n      <th>Number of websites</th>\n      <td>(1, 1, 1)</td>\n      <td>(1)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "third_parties_desktop = {'Third-party domain': stats_object_desktop['third_party_counts'].keys(), 'Number of websites': stats_object_desktop['third_party_counts'].values()}\n",
    "third_parties_mobile = {'Third-party domain': stats_object_mobile['third_party_counts'].keys(), 'Number of websites': stats_object_mobile['third_party_counts'].values()}\n",
    "\n",
    "\n",
    "third_parties_table = pd.DataFrame({'Crawl-desktop': third_parties_desktop,'Crawl-mobile': third_parties_mobile })\n",
    "third_parties_table"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e5ff1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#12\n",
    "canvas_data = []\n",
    "for crawl_data in []:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}